{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46e6820-b007-4e93-909a-494813e62f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading the KMNIST dataset...\n",
      "[INFO] generating the train/validation split...\n",
      "[INFO] initializing the LeNet model...\n",
      "[INFO] training the network...\n",
      "[INFO] EPOCH: 1/10\n",
      "Train loss: 0.350729, Train accuracy: 0.8915\n",
      "Val loss: 0.144253, Val accuracy: 0.9571\n",
      "\n",
      "[INFO] EPOCH: 2/10\n",
      "Train loss: 0.096532, Train accuracy: 0.9702\n",
      "Val loss: 0.090138, Val accuracy: 0.9733\n",
      "\n",
      "[INFO] EPOCH: 3/10\n",
      "Train loss: 0.056846, Train accuracy: 0.9826\n",
      "Val loss: 0.084208, Val accuracy: 0.9753\n",
      "\n",
      "[INFO] EPOCH: 4/10\n",
      "Train loss: 0.036603, Train accuracy: 0.9886\n",
      "Val loss: 0.073696, Val accuracy: 0.9793\n",
      "\n",
      "[INFO] EPOCH: 5/10\n",
      "Train loss: 0.024995, Train accuracy: 0.9924\n",
      "Val loss: 0.084698, Val accuracy: 0.9777\n",
      "\n",
      "[INFO] EPOCH: 6/10\n",
      "Train loss: 0.019655, Train accuracy: 0.9937\n",
      "Val loss: 0.069202, Val accuracy: 0.9822\n",
      "\n",
      "[INFO] EPOCH: 7/10\n",
      "Train loss: 0.014989, Train accuracy: 0.9950\n",
      "Val loss: 0.072567, Val accuracy: 0.9817\n",
      "\n",
      "[INFO] EPOCH: 8/10\n",
      "Train loss: 0.012505, Train accuracy: 0.9954\n",
      "Val loss: 0.088789, Val accuracy: 0.9798\n",
      "\n",
      "[INFO] EPOCH: 9/10\n",
      "Train loss: 0.011797, Train accuracy: 0.9962\n",
      "Val loss: 0.095311, Val accuracy: 0.9787\n",
      "\n",
      "[INFO] EPOCH: 10/10\n",
      "Train loss: 0.009017, Train accuracy: 0.9970\n",
      "Val loss: 0.089266, Val accuracy: 0.9827\n",
      "\n",
      "[INFO] total time taken to train the model: 168.52s\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           o       0.96      0.95      0.96      1000\n",
      "          ki       0.98      0.93      0.95      1000\n",
      "          su       0.91      0.91      0.91      1000\n",
      "         tsu       0.95      0.98      0.97      1000\n",
      "          na       0.96      0.93      0.95      1000\n",
      "          ha       0.97      0.94      0.95      1000\n",
      "          ma       0.93      0.97      0.95      1000\n",
      "          ya       0.93      0.98      0.95      1000\n",
      "          re       0.96      0.97      0.96      1000\n",
      "          wo       0.97      0.94      0.95      1000\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model output/model.pth --plot output/plot.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc5f555-29b9-43ef-bab1-54b860d25701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading kmnist test dataset...\n",
      "[INFO] ground truth label: ki, predicted label: ki\n",
      "[INFO] ground truth label: ki, predicted label: ki\n",
      "[INFO] ground truth label: ki, predicted label: ki\n",
      "[INFO] ground truth label: ha, predicted label: ha\n",
      "[INFO] ground truth label: tsu, predicted label: tsu\n",
      "[INFO] ground truth label: ya, predicted label: ya\n",
      "[INFO] ground truth label: tsu, predicted label: tsu\n",
      "[INFO] ground truth label: na, predicted label: na\n",
      "[INFO] ground truth label: ki, predicted label: ki\n",
      "[INFO] ground truth label: tsu, predicted label: tsu\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model output/model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dddd03c3-905c-4f16-bed6-bcac3710b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import calcConvOutDim, calcPoolOutDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44830bfc-8a4d-47c0-8963-74987a6216de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 4.0, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcConvOutDim(inputWidth=28, filters=20, fieldSize=5) # 24x24x20\n",
    "calcPoolOutDim(inputWidth=24, inputDepth=20) # 12x12x20\n",
    "calcConvOutDim(inputWidth=12, filters=50, fieldSize=5) # 8x8x50\n",
    "calcPoolOutDim(inputWidth=8, inputDepth=50) # 4x4x50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8127d1db-8d10-4f8d-ac1b-f7ad6af65198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training using cpu...\n",
      "[INFO] preparing data...\n",
      "Sequential(\n",
      "  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (activation_1): ReLU()\n",
      "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      "[INFO] epoch: 1\n",
      "epoch: 1 train loss: 0.923 train accuracy: 0.684\n",
      "epoch: 1 test loss: 0.516 test accuracy: 0.867\n",
      "\n",
      "[INFO] epoch: 2\n",
      "epoch: 2 train loss: 0.446 train accuracy: 0.914\n",
      "epoch: 2 test loss: 0.349 test accuracy: 0.947\n",
      "\n",
      "[INFO] epoch: 3\n",
      "epoch: 3 train loss: 0.310 train accuracy: 0.960\n",
      "epoch: 3 test loss: 0.254 test accuracy: 0.987\n",
      "\n",
      "[INFO] epoch: 4\n",
      "epoch: 4 train loss: 0.233 train accuracy: 0.976\n",
      "epoch: 4 test loss: 0.198 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 5\n",
      "epoch: 5 train loss: 0.187 train accuracy: 0.980\n",
      "epoch: 5 test loss: 0.162 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 6\n",
      "epoch: 6 train loss: 0.157 train accuracy: 0.980\n",
      "epoch: 6 test loss: 0.138 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 7\n",
      "epoch: 7 train loss: 0.136 train accuracy: 0.984\n",
      "epoch: 7 test loss: 0.121 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 8\n",
      "epoch: 8 train loss: 0.121 train accuracy: 0.985\n",
      "epoch: 8 test loss: 0.108 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 9\n",
      "epoch: 9 train loss: 0.109 train accuracy: 0.986\n",
      "epoch: 9 test loss: 0.099 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 10\n",
      "epoch: 10 train loss: 0.100 train accuracy: 0.987\n",
      "epoch: 10 test loss: 0.091 test accuracy: 0.993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run train_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df46f3b-8763-4008-9d46-e81786bdd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading vgg16...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'vgg16'...\n",
      "0, home_theater, home_theatre: 71.69%\n",
      "1, television, television_system: 5.22%\n",
      "2, cash_machine, cash_dispenser, automated_teller_machine, automatic_teller_machine, automated_teller, automatic_teller, ATM: 4.60%\n",
      "3, desktop_computer: 3.87%\n",
      "4, file, file_cabinet, filing_cabinet: 3.15%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/tv.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147b26a3-f5d2-4366-8e93-0d7c47c08fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading inception...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'inception'...\n",
      "0, home_theater, home_theatre: 94.42%\n",
      "1, television, television_system: 3.65%\n",
      "2, entertainment_center: 1.68%\n",
      "3, monitor: 0.15%\n",
      "4, desktop_computer: 0.02%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/tv.png --model inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9762b6-bfc1-4b74-8efc-8c05d17aab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading resnet...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'resnet'...\n",
      "0, home_theater, home_theatre: 86.81%\n",
      "1, television, television_system: 8.55%\n",
      "2, entertainment_center: 2.86%\n",
      "3, monitor: 1.15%\n",
      "4, screen, CRT_screen: 0.37%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/tv.png --model resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69eb996e-20ac-43e5-af15-23660238f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading densenet...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'densenet'...\n",
      "0, home_theater, home_theatre: 64.59%\n",
      "1, television, television_system: 28.48%\n",
      "2, entertainment_center: 3.74%\n",
      "3, monitor: 2.13%\n",
      "4, desktop_computer: 0.57%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/tv.png --model densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42d68a0-1917-459d-8df3-8d6e36188bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading vgg19...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'vgg19'...\n",
      "0, home_theater, home_theatre: 51.02%\n",
      "1, television, television_system: 9.22%\n",
      "2, desktop_computer: 7.18%\n",
      "3, monitor: 6.05%\n",
      "4, web_site, website, internet_site, site: 5.64%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/tv.png --model vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c825d47c-9e6c-4777-88d5-59025940f4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading inception...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'inception'...\n",
      "0, trimaran: 50.84%\n",
      "1, speedboat: 33.38%\n",
      "2, catamaran: 8.27%\n",
      "3, boathouse: 4.58%\n",
      "4, lifeboat: 0.57%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/beaver_lake_boat.png --model inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa092f2c-3f43-4bae-8a2f-049e0f29bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading inception...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n",
      "[INFO] classifying image with 'inception'...\n",
      "0, toilet_seat: 98.64%\n",
      "1, washbasin, handbasin, washbowl, lavabo, wash-hand_basin: 1.17%\n",
      "2, plunger, plumber's_helper: 0.11%\n",
      "3, toilet_tissue, toilet_paper, bathroom_tissue: 0.06%\n",
      "4, sewing_machine: 0.02%\n"
     ]
    }
   ],
   "source": [
    "%run classify_image.py --image images/emma_bathroom_nugget.png --model inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2af1da7-32c0-4944-9731-c85fe1baf6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and will be removed in 0.15, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] car: 99.54%\n",
      "[INFO] car: 99.18%\n",
      "[INFO] person: 85.76%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-resnet --image images/obj_detection/example_01.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75919d9e-05b0-4dfb-8238-dd8468cfe986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dog: 99.92%\n",
      "[INFO] person: 99.90%\n",
      "[INFO] chair: 99.42%\n",
      "[INFO] tv: 98.22%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-resnet --image images/obj_detection/example_06.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e826101-fd4c-49d9-b4b4-54edd0ba2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] horse: 99.89%\n",
      "[INFO] person: 99.75%\n",
      "[INFO] person: 99.03%\n",
      "[INFO] dog: 96.76%\n",
      "[INFO] person: 82.61%\n",
      "[INFO] person: 75.33%\n",
      "[INFO] truck: 68.75%\n",
      "[INFO] car: 66.92%\n",
      "[INFO] car: 64.58%\n",
      "[INFO] bench: 63.60%\n",
      "[INFO] person: 58.30%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-resnet --image images/obj_detection/example_05.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f32e443-a245-4285-9655-20558831523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] horse: 99.89%\n",
      "[INFO] person: 99.75%\n",
      "[INFO] person: 99.03%\n",
      "[INFO] dog: 96.76%\n",
      "[INFO] person: 82.61%\n",
      "[INFO] person: 75.33%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-resnet --image images/obj_detection/example_05.jpg --labels coco_classes.pickle --confidence 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207fc3a2-645e-48ee-a871-b91ddcd9a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] person: 99.95%\n",
      "[INFO] tv: 93.63%\n",
      "[INFO] dog: 53.74%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-mobilenet --image images/obj_detection/example_06.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87999309-79e9-4e30-bef7-ade73190810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] horse: 99.91%\n",
      "[INFO] person: 99.51%\n",
      "[INFO] dog: 91.52%\n",
      "[INFO] person: 82.52%\n",
      "[INFO] car: 64.44%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model frcnn-mobilenet --image images/obj_detection/example_05.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ae57aa-da8f-480a-b85d-11fa75eba9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dog: 96.88%\n",
      "[INFO] person: 94.90%\n",
      "[INFO] tv: 87.36%\n",
      "[INFO] chair: 86.24%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model retinanet --image images/obj_detection/example_06.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff216629-1796-4f81-8d7b-5fad5ce2ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] horse: 95.54%\n",
      "[INFO] dog: 93.63%\n",
      "[INFO] person: 90.33%\n",
      "[INFO] person: 83.68%\n",
      "[INFO] truck: 76.46%\n"
     ]
    }
   ],
   "source": [
    "%run detect_image.py --model retinanet --image images/obj_detection/example_05.jpg --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ae94c-fb9b-4283-a2f8-3737af24cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2844.297] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    }
   ],
   "source": [
    "%run detect_realtime.py --model frcnn-mobilenet --labels coco_classes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6be926-1872-43b5-8fab-a0b455fb783a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
